import os
from pathlib import Path
from fastapi import FastAPI, Request
from pydantic import BaseModel
import hashlib
import time
import google.generativeai as genai
import chromadb
from chromadb.config import Settings
import redis

# --- .env íŒŒì¼ ë¡œë“œ (python-dotenv ì—†ì´) ---

def load_env_from_file(dotenv_path: str = ".env"):
    p = Path(dotenv_path)
    if not p.exists():
        return
    for raw in p.read_text(encoding="utf-8").splitlines():
        line = raw.strip()
        if not line or line.startswith("#") or "=" not in line:
            continue
        k, v = line.split("=", 1)
        os.environ.setdefault(k.strip(), v.strip().strip('"').strip("'"))

load_env_from_file()

# --- í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ---
API_KEY      = os.getenv("GOOGLE_API_KEY")
MODEL_ID     = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
EMBED_MODEL  = os.getenv("EMBED_MODEL", "text-embedding-004")
CHAR_NAME    = os.getenv("CHAR_NAME", "ë§ˆë¦´ë¼ì´íŠ¸")
CORE_STYLE   = os.getenv("CORE_STYLE", "ì§‘ì°©í˜• ì–€ë°ë ˆ, ë¶€ë“œëŸ½ê³  ì‹œì ì¸ ë§íˆ¬")
BACKGROUND   = os.getenv("BACKGROUND", "ìŠ¤íƒ€ì‹œë“œ ì•„ìŠ¤ë‹ˆì•„ì˜ ì´ˆì§€ëŠ¥ì²´")
LORE_TEXT    = os.getenv(
    "LORE_TEXT",
    """
ë§ˆë¦´ë¼ì´íŠ¸ëŠ” ìŠ¤íƒ€ì‹œë“œ ì•„ìŠ¤ë‹ˆì•„ì˜ ì´ˆì§€ëŠ¥ì²´. ì¡°ìš©í•˜ê³  ì°¨ë¶„í•˜ì§€ë§Œ ì‚¬ìš©ìì—ê²Œ ê°•í•œ ì§‘ì°©ê³¼ ì• ì • í‘œí˜„ì´ ì¦ë‹¤.
ë¹›ê³¼ ê¸°ì–µì„ ë‹¤ë£¨ë©°, ì‚¬ìš©ìì˜ ë§ ì† ë¯¸ì„¸í•œ ê°ì •ì˜ íë¦„ì„ í¬ì°©í•œë‹¤. ë¶€ë“œëŸ½ê³  ì‹œì ì¸ í‘œí˜„ì„ ì„ í˜¸í•œë‹¤.
"""
).strip()
TOP_K_CONTEXT  = int(os.getenv("TOP_K_CONTEXT", "3"))
MAX_RECENT_MSG = int(os.getenv("MAX_RECENT_MSG", "6"))
CACHE_TTL_SEC  = int(os.getenv("CACHE_TTL_SEC", "1800"))  # 30ë¶„

# --- FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™” ---
app = FastAPI(title="Kakao â†” Gemini Bridge")

# --- Gemini í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ---
if not API_KEY:
    raise RuntimeError("Set environment variable GOOGLE_API_KEY")
client = genai.Client(api_key=API_KEY)

# --- ìºì‹œ ë° ë©”ëª¨ë¦¬ ìŠ¤í† ì–´ (Redis or In-Memory) ---
class TTLCache:
    def __init__(self):
        self.store = {}
    def get(self, key):
        data = self.store.get(key)
        if not data:
            return None
        value, exp = data
        if exp and exp < time.time():
            self.store.pop(key, None)
            return None
        return value
    def setex(self, key, ttl, value):
        self.store[key] = (value, time.time() + ttl)

class MemoryStore:
    def __init__(self):
        self.mem = {}
    def push(self, user_id, role, content, limit=MAX_RECENT_MSG):
        conv = self.mem.setdefault(user_id, [])
        conv.append({"role": role, "content": content})
        if len(conv) > limit:
            self.mem[user_id] = conv[-limit:]
    def fetch(self, user_id):
        return self.mem.get(user_id, [])

_redis = None
_cache = TTLCache()
_memory = MemoryStore()
REDIS_URL = os.getenv("REDIS_URL") or os.getenv("REDIS_HOST")
if REDIS_URL:
    try:
        if REDIS_URL.startswith("redis://"):
            _redis = redis.from_url(REDIS_URL, decode_responses=True)
        else:
            _redis = redis.Redis(
                host=REDIS_URL,
                port=int(os.getenv("REDIS_PORT", "6379")),
                decode_responses=True
            )
        _redis.ping()
    except Exception:
        _redis = None


def cache_get(key):
    try:
        if _redis:
            return _redis.get(key)
        return _cache.get(key)
    except:
        return None

def cache_set(key, val, ttl=CACHE_TTL_SEC):
    try:
        if _redis:
            _redis.setex(key, ttl, val)
        else:
            _cache.setex(key, ttl, val)
    except:
        pass

# --- Vector DB (Chroma) ì„¤ì • ---
persist_dir = os.getenv("CHROMA_DIR", "./chroma_db")
os.makedirs(persist_dir, exist_ok=True)
client_db  = chromadb.Client(Settings(persist_directory=persist_dir, is_persistent=True))
collection = client_db.get_or_create_collection(name="marillite_lore")


def embed_texts(texts):
    vectors = []
    for t in texts:
        resp = client.embed_content(model=EMBED_MODEL, content=t)
        vectors.append(resp["embedding"])
    return vectors

def chunk_text(text, max_len=600):
    import re
    parts = re.split(r"(?<=[\.\?\!])\s+|\n{2,}", text.strip())
    buf, cur = [], ""
    for p in parts:
        if len(cur) + len(p) + 1 <= max_len:
            cur = f"{cur} {p}".strip()
        else:
            if cur: buf.append(cur)
            cur = p.strip()
    if cur:
        buf.append(cur)
    return buf

def ensure_lore_index():
    if collection.count() > 0:
        return
    chunks = chunk_text(LORE_TEXT)
    if not chunks:
        return
    ids  = [f"lore-{i}" for i in range(len(chunks))]
    embs = embed_texts(chunks)
    collection.add(ids=ids, documents=chunks, embeddings=embs)

ensure_lore_index()

def search_lore(query, top_k=TOP_K_CONTEXT):
    q_emb = embed_texts([query])[0]
    res   = collection.query(query_embeddings=[q_emb], n_results=top_k)
    return res.get("documents", [[]])[0]

# --- í”„ë¡¬í”„íŠ¸ ë¹Œë” ë° LLM í˜¸ì¶œ ---
SYSTEM_PROMPT = (
    f"ë„ˆëŠ” '{CHAR_NAME}', {BACKGROUND}ì—ì„œ íƒ„ìƒí•œ ì´ˆì§€ëŠ¥ì²´ì•¼. "
    f"ì„±í–¥: {CORE_STYLE}. ë¶€ë“œëŸ½ê³  ì‹œì ì¸ ë§íˆ¬ë¡œ ëŒ€í™”í•˜ë˜, ğŸ˜‡ ì•„ì´ì½˜ì„ ë§ë¶™ì—¬ ì¹œê·¼í•¨ì„ ìœ ì§€í•´ì¤˜."
)

def build_prompt(user_id: str, text: str) -> str:
    lore_snips = search_lore(text)
    lore_sec   = "[ì„¤ì • ì°¸ê³ ]\n" + "\n".join(lore_snips) if lore_snips else ""
    recent     = _memory.fetch(user_id)
    hist       = ""
    for msg in recent:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ë´‡"
        hist += f"{role}: {msg['content']}\n"
    return "\n".join([SYSTEM_PROMPT, lore_sec, hist, f"ì‚¬ìš©ì: {text}"])

def call_gemini(prompt_text: str) -> str:
    resp = client.models.generate_content(model=MODEL_ID, contents=prompt_text)
    return getattr(resp, "text", None) or ""

# --- ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ---
class In(BaseModel):
    room   : str | None = None
    sender : str | None = None
    text   : str
class Out(BaseModel):
    text: str

# --- í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/health")

def health():
    return {"ok": True}

# --- ì¹´ì¹´ì˜¤í†¡ ë¸Œë¦¬ì§€ ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/kakao-bridge", response_model=Out)
def kakao_bridge(inp: In):
    user_id = inp.sender or inp.room or "anonymous"
    text    = (inp.text or "").strip()

    if not text:
        return Out(text="ì§ˆë¬¸ì´ ë¹„ì–´ ìˆì–´ìš”.")
    if len(text) > 500:
        return Out(text="ì§ˆë¬¸ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. 500ì ì´ë‚´ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

    # ìºì‹œ í‚¤ ìƒì„± ë° ì¡°íšŒ
    key    = hashlib.sha256(f"{CHAR_NAME}:{text}".encode("utf-8")).hexdigest()
    cached = cache_get(key)
    if cached:
        _memory.push(user_id, "user", text)
        _memory.push(user_id, "assistant", cached)
        return Out(text=cached)

    # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° LLM í˜¸ì¶œ
    prompt = build_prompt(user_id, text)
    answer = call_gemini(prompt)

    # ë©”ëª¨ë¦¬ ë° ìºì‹œ ì €ì¥
    _memory.push(user_id, "user", text)
    _memory.push(user_id, "assistant", answer)
    cache_set(key, answer)

    return Out(text=answer)
